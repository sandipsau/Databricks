{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20506874-c2ed-4255-8afb-0e8dd102d4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Manipulating DataFrame\n",
    "\n",
    "## Reference\n",
    "- https://spark.apache.org/docs/4.0.0/api/python/user_guide/dataprep.html\n",
    "\n",
    "## Create DataFrame\n",
    "\n",
    "- Example 1\n",
    "\n",
    "      from pyspark.sql import Row\n",
    "\n",
    "      df = spark.createDataFrame([\n",
    "          Row(age=10, height=80.0, NAME=\"Alice\"),\n",
    "          Row(age=10, height=80.0, NAME=\"Alice\"),\n",
    "          Row(age=5, height=float(\"nan\"), NAME=\"BOB\"),\n",
    "          Row(age=None, height=None, NAME=\"Tom\"),\n",
    "          Row(age=None, height=float(\"nan\"), NAME=None),\n",
    "          Row(age=9, height=78.9, NAME=\"josh\"),\n",
    "          Row(age=18, height=1802.3, NAME=\"bush\"),\n",
    "          Row(age=7, height=75.3, NAME=\"jerry\"),\n",
    "      ])\n",
    "\n",
    "      df.show()\n",
    "\n",
    "- Example 2\n",
    "\n",
    "      from pyspark.sql import SparkSession\n",
    "      from pyspark.sql.types import StructType, StructField, IntegerType, LongType, DoubleType, FloatType\n",
    "      from pyspark.sql.types import DecimalType, StringType, BinaryType, BooleanType, DateType, TimestampType\n",
    "      from decimal import Decimal\n",
    "      from datetime import date, datetime\n",
    "\n",
    "      # Define the schema of the DataFrame\n",
    "      schema = StructType([\n",
    "          StructField(\"integer_field\", IntegerType(), nullable=False),\n",
    "          StructField(\"long_field\", LongType(), nullable=False),\n",
    "          StructField(\"double_field\", DoubleType(), nullable=False),\n",
    "          StructField(\"float_field\", FloatType(), nullable=False),\n",
    "          StructField(\"decimal_field\", DecimalType(10, 2), nullable=False),\n",
    "          StructField(\"string_field\", StringType(), nullable=False),\n",
    "          StructField(\"binary_field\", BinaryType(), nullable=False),\n",
    "          StructField(\"boolean_field\", BooleanType(), nullable=False),\n",
    "          StructField(\"date_field\", DateType(), nullable=False),\n",
    "          StructField(\"timestamp_field\", TimestampType(), nullable=False)\n",
    "      ])\n",
    "\n",
    "      # Sample data using the Python objects corresponding to each PySpark type\n",
    "      data = [\n",
    "          (123, 1234567890123456789, 12345.6789, 123.456, Decimal('12345.67'), \"Hello, World!\",\n",
    "          b'Hello, binary world!', True, date(2020, 1, 1), datetime(2020, 1, 1, 12, 0)),\n",
    "          (456, 9223372036854775807, 98765.4321, 987.654, Decimal('98765.43'), \"Goodbye, World!\",\n",
    "          b'Goodbye, binary world!', False, date(2025, 12, 31), datetime(2025, 12, 31, 23, 59)),\n",
    "          (-1, -1234567890123456789, -12345.6789, -123.456, Decimal('-12345.67'), \"Negative Values\",\n",
    "          b'Negative binary!', False, date(1990, 1, 1), datetime(1990, 1, 1, 0, 0)),\n",
    "          (0, 0, 0.0, 0.0, Decimal('0.00'), \"\", b'', True, date(2000, 1, 1), datetime(2000, 1, 1, 0, 0))\n",
    "      ]\n",
    "\n",
    "      # Create DataFrame\n",
    "      df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "      # Show the DataFrame\n",
    "      df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5f7864-1ce4-474e-81ca-9dae00562d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the DataFrame from previous example in PySpark-Tutorial-1-Create DataFrame\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d980262-2a00-46d2-b5db-869a22cc4962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9761ed4d-bce5-442f-9756-73a54f247314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SELECT\n",
    "# Form 1\n",
    "df.select(\"Item_Identifier\", \"Item_Weight\", \"Item_Fat_Content\").display()\n",
    "# Form 2\n",
    "df.select(df[\"Item_Identifier\"], df[\"Item_Outlet_Sales\"]).display()\n",
    "# Form 3\n",
    "from pyspark.sql import functions as sf\n",
    "df.select(sf.col(\"Item_Identifier\"), sf.col(\"Outlet_Location_Type\")).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe4e4269-ca8d-4e72-bd8f-518e2529725f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ALIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1255a87e-f640-43ce-aba5-3348be1a5d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "df.select(sf.col(\"Item_Identifier\"), sf.col(\"Outlet_Location_Type\").alias(\"outlet_loc_type\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9103ea-82ec-40f0-9216-d661e40857b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Filter and where\n",
    "\n",
    "### Use Case 1: Filter the data with fat content = Regular\n",
    "### Use Case 2: Slice the data with item type = Soft Drinks and weight < 10\n",
    "### Use Case 3: Fetch the data with Tier in (Tier 1 or Tier 2) and Outlet Size is null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e54d16a-e865-4cf1-90e9-da1b7467840e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Case 1: Filter the data with fat content = Regular\n",
    "\n",
    "# df.display()\n",
    "# Form 1 - df.filter()\n",
    "# Form 2 - df.where()\n",
    "df.filter(sf.col(\"Item_Fat_Content\") == \"Regular\").display()\n",
    "df.where(sf.col(\"Item_Fat_Content\") == \"Regular\").display()\n",
    "\n",
    "# ==============================================================\n",
    "\n",
    "# Use Case 2: Slice the data with item type = Soft Drinks and weight < 10\n",
    "\n",
    "df.filter((sf.col(\"Item_Type\") == \"Soft Drinks\") & (sf.col(\"Item_Weight\") < 10)).display()\n",
    "\n",
    "# ==============================================================\n",
    "\n",
    "# Use Case 3: Fetch the data with Tier in (Tier 1 or Tier 2) and Outlet Size is null\n",
    "\n",
    "df.filter((sf.col(\"Outlet_Location_Type\").isin(\"Tier 1\", \"Tier 2\")) & (sf.col(\"Outlet_Size\").isNull())).orderBy(\"Outlet_Location_Type\").display()\n",
    "\n",
    "# =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35e77d0-56ba-4241-ada2-df5ac965e577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Rename Column(s)\n",
    "- df.withColumnRenamed() -- Rename single column\n",
    "- df.withColumnsRenamed() -- Rename multiple columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2b4d3f-232a-4588-a60e-b2d48ac5004f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumnRenamed(\"Item_Identifier\", \"item_id\").display() # Rename single column\n",
    "\n",
    "df.withColumnsRenamed({\"Item_Identifier\": \"item_id\", \"Item_Weight\": \"item_weight\", \"Item_Fat_Content\": \"item_fat_content\"}).display() # Rename multiple columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2cb54eb-0e50-4e6f-8af5-f1cd43be9984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# With Column function\n",
    "- Create new column in the DataFrame\n",
    "\n",
    "      Use lit() to set constant values to a new column\n",
    "\n",
    "- Modify existing column\n",
    "      \n",
    "      Usage of when(), regexp_replace() with when()\n",
    "      Case insensitive (?i)\n",
    "      Not Null using isNotNull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b3b1e3-0049-462b-9671-9d73ff4ae47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.withColumn(\"flag\", sf.lit(\"new\")).display() \n",
    "# Creating a new column in df with name \"flag\" with a constant value \"new\" using lit() function\n",
    "\n",
    "df.withColumn(\"item_weight_mrp_product\", (sf.col(\"Item_Weight\") * sf.col(\"Item_MRP\"))).display() \n",
    "# Creating a new column in df with name \"item_weight_mrp_product\" with a value by multiplying 2 columns\n",
    "\n",
    "df.withColumn(\"item_fat_content_accronym\", sf.when(sf.col(\"Item_Fat_Content\") == \"Low Fat\", \"LF\")\n",
    "              .when(sf.col(\"Item_Fat_Content\") == \"Regular\", \"REG\")\n",
    "              .otherwise(sf.col(\"Item_Fat_Content\"))).display() \n",
    "# Creating a new column in df with name \"item_fat_content_accronym\" with a value by using when() function to map values from existing value to a new value. In this case \"Low Fat\" is mapped to \"LF\" and \"Regular\" is mapped to \"REG\".\n",
    "\n",
    "df.withColumn(\n",
    "    \"Item_Fat_Content\", \n",
    "    sf.when(sf.col(\"Item_Fat_Content\").isNotNull(), \n",
    "        sf.regexp_replace(sf.col(\"Item_Fat_Content\"), \"(?i)Regular\", \"Reg\")\n",
    "    ).otherwise(sf.col(\"Item_Fat_Content\"))\n",
    "    ).withColumn(\"Item_Fat_Content\", \n",
    "            sf.when(sf.col(\"Item_Fat_Content\").isNotNull(),\n",
    "                    sf.regexp_replace(sf.col(\"Item_Fat_Content\"), \"(?i)Low Fat\", \"Lf\")\n",
    "            ).otherwise(sf.col(\"Item_Fat_Content\"))\n",
    "    ).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "084ca1df-6313-4c5f-b266-19c16db9dfba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Type Casting\n",
    "\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql import functions as sf\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "    # Define the schema of the DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"float_column\", FloatType(), nullable=True),\n",
    "        StructField(\"string_column\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Sample data\n",
    "    data = [\n",
    "        (123.456, \"123\"),\n",
    "        (789.012, \"456\"),\n",
    "        (None, \"789\")\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    # Display original DataFrame\n",
    "    print(\"Original DataFrame:\")\n",
    "    df.show()\n",
    "\n",
    "    # Example of casting a float column to string\n",
    "    df = df.withColumn('string_from_float', col('float_column').cast('string'))\n",
    "\n",
    "    # Example of casting a string column to integer\n",
    "    df = df.withColumn('integer_from_string', col('string_column').cast('integer'))\n",
    "\n",
    "    # Example of modifiying the type of an existing column by casting\n",
    "    df = df.withColumn(sf.col(\"Item_Weight\"), sf.col('Item_Weight').cast(sf.StringType()))\n",
    "\n",
    "    # Display DataFrame after casting\n",
    "    print(\"DataFrame after Casting:\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb64034c-b102-4307-b626-56ed14fef7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "df = df.withColumn('Item_Weight', sf.col('Item_Weight').cast(StringType())) # Using StringType() function to cast the column to string\n",
    "display(df)\n",
    "\n",
    "df = df.withColumn('Item_Weight', sf.col('Item_Weight').cast(DoubleType()))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17039980-97fb-4b51-809c-f2151c43121e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SORT/ORDER BY\n",
    "\n",
    "      df.sort(sf.col('col_name').asc()) # Sorting a column in ascending order\n",
    "\n",
    "      df.sort(sf.col('col_name').desc()) # Sorting a column in descending order\n",
    "\n",
    "      df.sort([sf.col('col_name_1'), sf.col('col_name_2')], ascending = [0,0]) # Sorting both the columns in descending example of 2 stage sorting\n",
    "\n",
    "      df.sort([sf.col('col_name_1'), sf.col('col_name_2')], ascending = [0,1]) # Sorting first column descending and second ascending example of 2 stage sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c325d22-7090-4fa5-baeb-3e2e74cfea6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "df.sort(sf.col('Item_Weight').desc()).display()\n",
    "df.sort(sf.col('Item_Weight').asc()).display()\n",
    "df.sort([sf.col('Item_Weight'), sf.col('Item_MRP')], ascending = [0,0]).display()\n",
    "df.sort([sf.col('Item_Weight'), sf.col('Item_MRP')], ascending = [0,1]).display()\n",
    "df.orderBy(sf.col('Item_Weight').desc(), sf.col('Item_MRP').desc()).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c387c824-36ad-4782-894c-6099fe2e1bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LIMIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c761718-f4cd-4369-9f66-63a2de455815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1af92c2b-c400-4567-8a38-2c76f79436d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e29077c-9fbc-4d2d-8c46-fbf66643eb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.drop(sf.col(\"Item_Identifier\")).display() # Drop single column\n",
    "df.drop(sf.col(\"Item_Identifier\"), sf.col(\"Item_Weight\")).display() # Drop multiple columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d1f4479-d2d7-40ec-a0b8-862a4f7c568e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Drop_Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a992e97-c2a5-486a-847b-983b3190b95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates().display() # Drop duplicate rows\n",
    "df.dropDuplicates([\"Item_Identifier\"]).display()\n",
    "df.dropDuplicates(subset=[\"Item_Identifier\"]).display()\n",
    "df.distinct().display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark-Tutorial-2-Manipulate DataFrame",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
