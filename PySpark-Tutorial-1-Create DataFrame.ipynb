{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ad33e4f-6aa9-4b98-bf4d-4d86392fbbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference Documentation\n",
    "\n",
    "- [https://spark.apache.org/docs/4.0.0/api/python/user_guide/dataframes.html?highlight=inferschema](Apache Spark User Guide)\n",
    "\n",
    "## Load Sample Data\n",
    "\n",
    "- Use the option in Catalog to Load Sample Data file from local/S3/any Cloud Storage \n",
    "\n",
    "## Verify the contents of the data load location\n",
    "\n",
    "  `dbutils.fs.ls(\"dbfs:/Volumes/workspace/default/tutorial\")`\n",
    "\n",
    "## What is DataFrame\n",
    "\n",
    "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames support a rich set of APIs (select columns, filter, join, aggregate, etc.) that allow you to solve common data analysis problems efficiently.\n",
    "\n",
    "Compared to traditional relational databases, Spark DataFrames offer several key advantages for big data processing and analytics:\n",
    "\n",
    "- **Distributed computing**: Spark distributes data across multiple nodes in a cluster, allowing for parallel processing of big data\n",
    "- **In-memory processing**: Spark performs computations in memory, which can be significantly faster than disk-based processing\n",
    "- **Schema flexibility**: Unlike traditional databases, PySpark DataFrames support schema evolution and dynamic typing\n",
    "- **Fault tolerance**: PySpark DataFrames are built on top of Resilient Distributed Dataset (RDDs), which are inherently fault-tolerant. Spark automatically handles node failures and data replication, ensuring data reliability and integrity.\n",
    "\n",
    "\n",
    "## Create Dataframe\n",
    "\n",
    "- **From a list of dictionaries**:\n",
    "\n",
    "  employees = [{\"name\": \"John D.\", \"age\": 30},\n",
    "    {\"name\": \"Alice G.\", \"age\": 25},\n",
    "    {\"name\": \"Bob T.\", \"age\": 35},\n",
    "    {\"name\": \"Eve A.\", \"age\": 28}]\n",
    "\n",
    "  **`Create a DataFrame containing the employees data: `**\n",
    "  \n",
    "    `df = spark.createDataFrame(employees)`\n",
    "    \n",
    "    `df.show()`\n",
    "\n",
    "- **From a local file**:\n",
    "\n",
    "  `df = spark.read.csv(\"../data/employees.csv\", header=True, inferSchema=True)`\n",
    "  \n",
    "  `df.show()`\n",
    "\n",
    "- **From a local json**:\n",
    "\n",
    "  `df = spark.read.option(\"multiline\",\"true\").json(\"../data/employees.json\")`\n",
    "  \n",
    "  `df.show()`\n",
    "\n",
    "- **From an existing DataFrame**:\n",
    "\n",
    "  employees = [\n",
    "    {\"name\": \"John D.\", \"age\": 30, \"department\": \"HR\"},\n",
    "    {\"name\": \"Alice G.\", \"age\": 25, \"department\": \"Finance\"},\n",
    "    {\"name\": \"Bob T.\", \"age\": 35, \"department\": \"IT\"},\n",
    "    {\"name\": \"Eve A.\", \"age\": 28, \"department\": \"Marketing\"}\n",
    "  ]\n",
    "  \n",
    "  `df = spark.createDataFrame(employees)`\n",
    "\n",
    "  **`Select only the name and age columns: `**\n",
    "  `new_df = df.select(\"name\", \"age\")`\n",
    "\n",
    "- **From a table in Spark environment**:\n",
    "\n",
    "  `df = spark.read.table(\"table_name\")  `\n",
    "\n",
    "- **From a table in an external Database, by connecting using JDBC to read the table into DataFrame**:\n",
    "\n",
    "  url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "  table = \"employees\"\n",
    "  properties = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\"\n",
    "  }\n",
    "\n",
    "  **`Read table into DataFrame: `**\n",
    "  `df = spark.read.jdbc(url=url, table=table, properties=properties)`\n",
    "\n",
    "## Display DataFrame\n",
    "\n",
    "- **df.show()** - Displays the basic visualization of the DataFrame's contents. By default it displays first 20 rows.\n",
    "\n",
    "- **df.show(n=2)** - Displays only 2 rows\n",
    "    +---+--------+\n",
    "    |age|    name|\n",
    "    +---+--------+\n",
    "    | 30| John D.|\n",
    "    | 25|Alice G.|\n",
    "    +---+--------+\n",
    "    only showing top 2 rows\n",
    "\n",
    "- **df.show(truncate=3)** - Truncate attribute controls the length of the displayed column values, by default it's 20\n",
    "    +---+----+\n",
    "    |age|name|\n",
    "    +---+----+\n",
    "    | 30| Joh|\n",
    "    | 25| Ali|\n",
    "    | 35| Bob|\n",
    "    | 28| Eve|\n",
    "    +---+----+\n",
    "\n",
    "- **df.show(vertical=True)** - DataFrame will be displayed vertically with one line per value\n",
    "    -RECORD 0--------\n",
    "    age  | 30\n",
    "    name | John D.\n",
    "    -RECORD 1--------\n",
    "    age  | 25\n",
    "    name | Alice G.\n",
    "    -RECORD 2--------\n",
    "    age  | 35\n",
    "    name | Bob T.\n",
    "    -RECORD 3--------\n",
    "    age  | 28\n",
    "    name | Eve A.\n",
    "\n",
    "- **df.printSchema()** - To view the schema of the DataFrame\n",
    "\n",
    "- **display(df)** (Only Available in Databricks): Displays the data in tabular format.\n",
    "\n",
    "- **display(df, streamName)** (Only Available in Databricks): Render streaming data in real-time.\n",
    "  \n",
    "    display(df, streamName=\"myStream\")\n",
    "\n",
    "  - Shows live updates of a structured streaming query.\n",
    "  - Automatically refreshes until you stop the cell.\n",
    "  Example:\n",
    "\n",
    "  `streamingDF = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"/path\")`\n",
    "\n",
    "  `display(streamingDF, streamName=\"LiveCSVStream\")`\n",
    "\n",
    "- **display(df.select(\"col1\", \"col2\"))** (Only Available in Databricks): Display only selected columns\n",
    "\n",
    "- **display with Visualization options** (Only Available in Databricks): After calling display(df) in a Databricks notebook, we can switch between Table, Bar chart, Line chart, Scatter plot, Map, etc.\n",
    "\n",
    "- **display with Temporary SQL Views** (Only Available in Databricks): You can combine with createOrReplaceTempView() to display SQL query results:\n",
    "\n",
    "  `df.createOrReplaceTempView(\"my_table\")`\n",
    "  \n",
    "  `display(spark.sql(\"SELECT col1, count(*) FROM my_table GROUP BY col1\"))`\n",
    "\n",
    "\n",
    "## DataFrame Manipulation\n",
    "\n",
    "### Reference: \n",
    "- [https://spark.apache.org/docs/4.0.0/api/python/user_guide/dataprep.html](Apache Spark User Guide - Data Frame Manipulation)\n",
    "- PySpark-Tutorial-2-Manipulate DataFrame Notebook\n",
    "\n",
    "## DataFrame v/s Tables\n",
    "\n",
    "- **DataFrame**: A DataFrame is an immutable distributed collection of data, only available in the **current Spark** session.\n",
    "- **Tables**: A table is a persistent data structure that can be accessed across **multiple Spark** sessions.\n",
    "- **Convert DataFrame to Table**: df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "  (Note: The lifetime of this **temporary table** is tied to the **Spark session** that was used to **create this DataFrame**. To persist the table **beyond this Spark session**, you will need to save it to **persistent storage**.)\n",
    "\n",
    "## Save DataFrame to Persisted Storage\n",
    "\n",
    "- **Save to file based Data Store**: df.write.option(\"path\", \"../dataout\").saveAsTable(\"dataframes_savetable_example\")\n",
    "\n",
    "  For file-based data source (text, parquet, json, etc.), you can specify a custom table path. Even if the table is dropped, the custom table path and table data will still be there.\n",
    "\n",
    "  If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. When the table is dropped, the default table path will be removed too.\n",
    "\n",
    "- **Save to Hive metastore**: df.write().mode(\"overwrite\").saveAsTable(\"schemaName.tableName\")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132f4472-3807-4c42-aae4-480544376a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"dbfs:/Volumes/workspace/default/tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd30c04-718c-490a-91a1-5a701df7db60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, inferSchema=True)\n",
    "df.show(n=3, vertical=True)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff5224d-b57d-4fb0-acd7-d50257bddd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0324e5-dbfa-454b-995e-6678da78426d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfj = spark.read.option(\"multiline\", False).json(\"/Volumes/workspace/default/tutorial/drivers.json\")\n",
    "display(dfj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a903000c-81b2-4cef-ab20-f9e3c4462aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manipulating Schema Using DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc7e6d0-3a12-4141-a6eb-2d61a537c4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75ae954-5d77-40f9-9e60-78bd86f5b3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_modified_ddl_schema = \"\"\"\n",
    "                      Item_Identifier     STRING,\n",
    "                      Item_Weight     STRING,\n",
    "                      Item_Fat_Content  STRING,\n",
    "                      Item_Visibility   DOUBLE,\n",
    "                      Item_Type         STRING,\n",
    "                      Item_MRP          DOUBLE,\n",
    "                      Outlet_Identifier STRING,\n",
    "                      Outlet_Establishment_Year INTEGER,\n",
    "                      Outlet_Size       STRING,\n",
    "                      Outlet_Location_Type STRING,\n",
    "                      Outlet_Type       STRING,\n",
    "                      Item_Outlet_Sales DOUBLE\n",
    "                      \"\"\"\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, schema=new_modified_ddl_schema)\n",
    "df.printSchema()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a66cb5fc-b4b4-415a-998d-0112ae7b1d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manipulating Schema Using Struct Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2918b2f-8728-4b12-935d-405adb55afdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "new_modified_struct_schema = StructType([\n",
    "    StructField(\"Item_Identifier\", StringType(), True),\n",
    "    StructField(\"Item_Weight\", StringType(), True),\n",
    "    StructField(\"Item_Fat_Content\", StringType(), True),\n",
    "    StructField(\"Item_Visibility\", DoubleType(), True),\n",
    "    StructField(\"Item_Type\", StringType(), True),\n",
    "    StructField(\"Item_MRP\", DoubleType(), True),\n",
    "    StructField(\"Outlet_Identifier\", StringType(), True),\n",
    "    StructField(\"Outlet_Establishment_Year\", IntegerType(), True),\n",
    "    StructField(\"Outlet_Size\", StringType(), True),\n",
    "    StructField(\"Outlet_Location_Type\", StringType(), True),\n",
    "    StructField(\"Outlet_Type\", StringType(), True),\n",
    "    StructField(\"Item_Outlet_Sales\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/tutorial/BigMart Sales.csv\", header=True, schema=new_modified_struct_schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57535237-a5d4-4ad6-89e1-bd3b0259671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create DataFrame with schema containing different data types\n",
    "\n",
    "### Reference\n",
    "- https://spark.apache.org/docs/4.0.0/api/python/user_guide/touroftypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7640f6f4-4c08-48ef-8417-08c71553a2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, DoubleType, FloatType\n",
    "from pyspark.sql.types import DecimalType, StringType, BinaryType, BooleanType, DateType, TimestampType\n",
    "from decimal import Decimal\n",
    "from datetime import date, datetime\n",
    "\n",
    "# Define the schema of the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"integer_field\", IntegerType(), nullable=False),\n",
    "    StructField(\"long_field\", LongType(), nullable=False),\n",
    "    StructField(\"double_field\", DoubleType(), nullable=False),\n",
    "    StructField(\"float_field\", FloatType(), nullable=False),\n",
    "    StructField(\"decimal_field\", DecimalType(10, 2), nullable=False),\n",
    "    StructField(\"string_field\", StringType(), nullable=False),\n",
    "    StructField(\"binary_field\", BinaryType(), nullable=False),\n",
    "    StructField(\"boolean_field\", BooleanType(), nullable=False),\n",
    "    StructField(\"date_field\", DateType(), nullable=False),\n",
    "    StructField(\"timestamp_field\", TimestampType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Sample data using the Python objects corresponding to each PySpark type\n",
    "data = [\n",
    "    (123, 1234567890123456789, 12345.6789, 123.456, Decimal('12345.67'), \"Hello, World!\",\n",
    "     b'Hello, binary world!', True, date(2020, 1, 1), datetime(2020, 1, 1, 12, 0)),\n",
    "    (456, 9223372036854775807, 98765.4321, 987.654, Decimal('98765.43'), \"Goodbye, World!\",\n",
    "     b'Goodbye, binary world!', False, date(2025, 12, 31), datetime(2025, 12, 31, 23, 59)),\n",
    "    (-1, -1234567890123456789, -12345.6789, -123.456, Decimal('-12345.67'), \"Negative Values\",\n",
    "     b'Negative binary!', False, date(1990, 1, 1), datetime(1990, 1, 1, 0, 0)),\n",
    "    (0, 0, 0.0, 0.0, Decimal('0.00'), \"\", b'', True, date(2000, 1, 1), datetime(2000, 1, 1, 0, 0))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_multi_data_type = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show schema\n",
    "df_multi_data_type.printSchema()\n",
    "\n",
    "# Show the DataFrame\n",
    "df_multi_data_type.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark-Tutorial-1-Create DataFrame",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
