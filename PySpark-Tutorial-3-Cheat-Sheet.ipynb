{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb2ec83-f734-48fd-a4ee-6cff6459dc56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Setup & Session\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = (SparkSession.builder\n",
    "            .appName(\"app\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "            .getOrCreate())\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "## Common Configs\n",
    "\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "    spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "\n",
    "## Data I/O (Delta/Parquet/CSV/JSON/SQL)\n",
    "\n",
    "    # Read\n",
    "    df = spark.read.format(\"delta\").load(\"/path\")\n",
    "    df = spark.read.parquet(\"/path\")\n",
    "    df = spark.read.csv(\"/path\", header=True, inferSchema=True)\n",
    "    df = spark.read.json(\"/path\")\n",
    "    df = spark.read.table(\"catalog.db.table\")          # Unity Catalog or hive_metastore\n",
    "    df = spark.sql(\"SELECT * FROM catalog.db.table\")   # SQL\n",
    "\n",
    "    # Write (overwrite/append + partitioning)\n",
    "    (df.write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .partitionBy(\"dt\")\n",
    "      .save(\"/path\"))\n",
    "\n",
    "    df.write.saveAsTable(\"catalog.db.table\", format=\"delta\", mode=\"append\")\n",
    "\n",
    "## DataFrame Basics\n",
    "    df.printSchema()\n",
    "    df.show(20, truncate=False)\n",
    "    df.head(1)                  # collect small\n",
    "    df.count()                  # action\n",
    "    df.cache(); df.count()      # materialize\n",
    "    df.unpersist()\n",
    "    df.explain(mode=\"formatted\")\n",
    "\n",
    "- **Selecting & Renaming**\n",
    "\n",
    "      from pyspark.sql.functions import col, expr\n",
    "\n",
    "      df.select(\"a\", col(\"b\").alias(\"b1\"), expr(\"a + b as a_plus_b\"))\n",
    "      df.withColumnRenamed(\"old\", \"new\")\n",
    "      df.drop(\"unwanted_col\")\n",
    "\n",
    "- **Filtering & Ordering**  \n",
    "      \n",
    "      df.filter(col(\"x\") > 5)                 # or df.where(\"x > 5\")\n",
    "      df.orderBy(col(\"dt\").desc(), \"id\")\n",
    "      df.dropDuplicates([\"key1\",\"key2\"])      # distinct on keys\n",
    "\n",
    "- **Joins**\n",
    "\n",
    "      df.join(df2, on=\"id\", how=\"inner\")\n",
    "      df.join(df2, (df.id == df2.id) & (df.dt == df2.dt), \"left\")\n",
    "      df.hint(\"broadcast\").join(small_df, \"key\", \"left\")\n",
    "\n",
    "- **Aggregations & Grouping**\n",
    "\n",
    "      from pyspark.sql.functions import sum as _sum, avg, countDistinct\n",
    "\n",
    "      df.groupBy(\"k\").agg(_sum(\"amt\").alias(\"total_amt\"), avg(\"score\"))\n",
    "      df.rollup(\"year\",\"month\").sum(\"sales\")\n",
    "      df.cube(\"a\",\"b\").count()\n",
    "\n",
    "- **Columns, Expressions & Functions**\n",
    "\n",
    "      from pyspark.sql.functions import (\n",
    "        lit, when, coalesce, greatest, least, isnan, isnull, round, expr\n",
    "      )\n",
    "\n",
    "      df.select(when(col(\"x\") > 0, \"pos\").otherwise(\"neg\").alias(\"sign\"))\n",
    "      df.select(coalesce(\"a\",\"b\",\"c\").alias(\"first_non_null\"))\n",
    "      df.select(greatest(\"a\",\"b\",\"c\").alias(\"maxval\"))\n",
    "      df.filter(isnull(\"a\") | isnan(\"a\"))\n",
    "      df.select(round(col(\"price\"), 2))\n",
    "      df.select(expr(\"case when a>0 then 1 else 0 end\").alias(\"flag\"))\n",
    "\n",
    "- **Strings / Dates / Arrays / Maps**\n",
    "\n",
    "      from pyspark.sql.functions import (\n",
    "        upper, lower, trim, regexp_extract, regexp_replace, length, concat_ws,\n",
    "        to_date, to_timestamp, date_add, date_sub, current_date, datediff,\n",
    "        split, size, array, explode, map_from_arrays\n",
    "      )\n",
    "\n",
    "      # Strings\n",
    "      df.select(upper(\"name\"), regexp_replace(\"email\", \"@.*\", \"@mask.com\"))\n",
    "\n",
    "      # Dates\n",
    "      df.select(to_date(\"dt_str\",\"yyyy-MM-dd\").alias(\"dt\"),\n",
    "                datediff(current_date(), col(\"dt\")).alias(\"age_days\"))\n",
    "\n",
    "      # Arrays & explode\n",
    "      df.select(split(\"csv\",\"\\\\,\").alias(\"arr\")).select(explode(\"arr\").alias(\"val\"))\n",
    "\n",
    "      # Maps\n",
    "      df.select(map_from_arrays(col(\"keys\"), col(\"vals\")).alias(\"m\"))\n",
    "\n",
    "## Window Functions\n",
    "\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, sum as _sum\n",
    "\n",
    "    w = Window.partitionBy(\"part\").orderBy(col(\"ts\").desc())\n",
    "\n",
    "    df.select(\"id\",\"part\",\"ts\",\n",
    "              row_number().over(w).alias(\"rn\"),\n",
    "              lag(\"value\", 1).over(w).alias(\"prev_val\"),\n",
    "              _sum(\"value\").over(w.rowsBetween(Window.unboundedPreceding, Window.currentRow)).alias(\"running_sum\"))\n",
    "\n",
    "## Nulls & Conditionals\n",
    "\n",
    "    df.fillna({\"str_col\": \"N/A\", \"num_col\": 0})\n",
    "    df.na.drop(subset=[\"must_have\"])\n",
    "    df.na.replace({\"old\": \"new\"}, subset=[\"col\"])\n",
    "\n",
    "## UDFs & Pandas UDFs\n",
    "\n",
    "    from pyspark.sql.types import IntegerType\n",
    "    from pyspark.sql.functions import udf\n",
    "\n",
    "    @udf(IntegerType())\n",
    "    def add_one(x): return None if x is None else x + 1\n",
    "\n",
    "    df.select(add_one(\"n\").alias(\"n1\"))\n",
    "\n",
    "- **Pandas UDF (vectorized; faster)**\n",
    "\n",
    "      import pandas as pd\n",
    "      import pyspark.sql.functions as F\n",
    "      from pyspark.sql.types import LongType\n",
    "\n",
    "      @F.pandas_udf(LongType())\n",
    "      def add_two_udf(s: pd.Series) -> pd.Series:\n",
    "          return s + 2\n",
    "\n",
    "      df.select(add_two_udf(\"n\").alias(\"n2\"))\n",
    "\n",
    "## Performance Essentials\n",
    "\n",
    "    df.repartition(200, \"key\")     # wider shuffle by key\n",
    "    df.coalesce(10)                # narrow reduce partitions\n",
    "    small = F.broadcast(small_df)  # force broadcast\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50*1024*1024)\n",
    "\n",
    "- Prefer built-in functions over UDFs (Catalyst optimization).\n",
    "- Use Delta + Z-Ordering for skip scanning; vacuum old files.\n",
    "- Enable AQE (adaptive query execution).\n",
    "- Cache only if reused; unpersist when done.\n",
    "\n",
    "## Structured Streaming (Core Pattern)\n",
    "\n",
    "    from pyspark.sql.functions import expr\n",
    "\n",
    "    stream_df = (spark.readStream\n",
    "                .format(\"cloudFiles\")              # Databricks Autoloader, if used\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .load(\"/input\"))\n",
    "\n",
    "    transformed = stream_df.selectExpr(\"cast(value as string) as v\")\n",
    "\n",
    "    query = (transformed.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", \"/chk/stream1\")\n",
    "            .outputMode(\"append\")\n",
    "            .start(\"/output\"))\n",
    "\n",
    "    # query.status, query.lastProgress\n",
    "    # query.stop()\n",
    "- Databricks preview:\n",
    "\n",
    "    display(stream_df, streamName=\"live\")\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "    df.createOrReplaceTempView(\"t\")\n",
    "    spark.sql(\"\"\"\n",
    "      SELECT k, SUM(v) AS s\n",
    "      FROM t\n",
    "      WHERE dt >= '2025-01-01'\n",
    "      GROUP BY k\n",
    "      HAVING s > 100\n",
    "      ORDER BY s DESC\n",
    "    \"\"\")\n",
    "\n",
    "## RDD (when you really need it)\n",
    "\n",
    "    r = sc.parallelize([1,2,3,4])\n",
    "    r.map(lambda x: x*x).filter(lambda x: x>4).collect()\n",
    "\n",
    "## MLlib (Pipelines Quickstart)\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "    idx = StringIndexer(inputCol=\"label_str\", outputCol=\"label\")\n",
    "    vec = VectorAssembler(inputCols=[\"f1\",\"f2\",\"f3\"], outputCol=\"features\")\n",
    "    lr  = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[idx, vec, lr])\n",
    "    model = pipeline.fit(train_df)\n",
    "    pred  = model.transform(test_df)\n",
    "\n",
    "## Databricks-Specific Nice-to-Haves\n",
    "\n",
    "    display(df)                              # interactive\n",
    "    dbutils.fs.ls(\"/mnt/...\")                # list files\n",
    "    dbutils.widgets.text(\"p_dt\",\"\")          # widgets\n",
    "    dbutils.jobs.taskValues.set(key=\"k\", value=\"v\")   # job orchestration\n",
    "\n",
    "## Delta Lake Operations\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    delta = DeltaTable.forPath(spark, \"/path\")\n",
    "\n",
    "    # Upsert (MERGE)\n",
    "    delta.alias(\"t\").merge(\n",
    "      source=df.alias(\"s\"),\n",
    "      condition=\"t.id = s.id\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    # Time travel\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"/path\")\n",
    "\n",
    "    # Optimize & Vacuum (Databricks SQL commands)\n",
    "    spark.sql(\"OPTIMIZE catalog.db.table ZORDER BY (id)\")\n",
    "    spark.sql(\"VACUUM catalog.db.table RETAIN 168 HOURS\")\n",
    "\n",
    "## Testing & Validation\n",
    "\n",
    "    assert df.filter(col(\"id\").isNull()).count() == 0\n",
    "    assert set(df.columns) >= {\"id\",\"dt\",\"amount\"}\n",
    "    df.limit(10).toPandas()  # small sample to inspect\n",
    "\n",
    "## Debug & Explain\n",
    "\n",
    "    df.explain(True)                 # or \"cost\", \"extended\", \"formatted\"\n",
    "    spark.catalog.listTables(\"db\")\n",
    "    spark.catalog.listColumns(\"db\", \"table\")\n",
    "\n",
    "## Handy One-Liners\n",
    "\n",
    "    # Schema DDL\n",
    "    df.printSchema(); print(df._jdf.schema().treeString())  # or df.schema.simpleString()\n",
    "\n",
    "    # Flatten nested struct fields (example)\n",
    "    from pyspark.sql.functions import col\n",
    "    flat = df.select(*[col(f\"{c}.{a}\").alias(f\"{c}_{a}\") \n",
    "                      for c,t in df.dtypes if t.startswith(\"struct\") \n",
    "                      for a,_ in spark.createDataFrame([], df.schema[c].dataType).dtypes])\n",
    "\n",
    "## Version Checks\n",
    "\n",
    "    import pyspark, platform\n",
    "    print(pyspark.__version__, spark.version, platform.python_version())\n",
    "\n",
    "## Typical Project Layout\n",
    "\n",
    "    project/\n",
    "      ├─ notebooks/             # exploration\n",
    "      ├─ jobs/                  # scheduled entry points\n",
    "      ├─ src/                   # libs (transformations, utils)\n",
    "      ├─ tests/                 # unit tests with pytest + chispa\n",
    "      └─ conf/                  # job configs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1144d781-1e51-4b40-b0fc-cfd8c6de1fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark-Tutorial-3-Cheat-Sheet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
